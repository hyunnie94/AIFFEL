{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-7. 프로젝트: 멋진 작사가 만들기\n",
    "\n",
    "### [루브릭]\n",
    "평가문항\t상세기준\n",
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?  \n",
    "  -텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?  \n",
    "      \n",
    "      \n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?  \n",
    "  -특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?  \n",
    "     \n",
    "     \n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?  \n",
    "  -텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?  \n",
    "    \n",
    "### [학습 과정]\n",
    "1. 데이터 다운로드\n",
    "2. 데이터 읽어오기\n",
    "3. 데이터 정제\n",
    "4. 평가 데이터셋 분리\n",
    "5. 인공지능 만들기\n",
    "\n",
    "### [결과 및 회고]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free', 'They come to join us here', 'From sea to shining sea And they all have a dream', 'As people always will', 'To be safe and warm', 'In that shining city on the hill Some wanna slam the door', 'Instead of opening the gate', \"Aw, let's turn this thing around\", 'Before it gets too late [Chorus]', \"It's up to me and you\", 'Love can conquer hate', 'I know this to be true', \"That's what makes us great [Verse 2]\", \"Don't tell me a lie\", 'And sell it as a fact', \"I've been down that road before\", \"And I ain't goin' back And don't you brag to me\", 'That you never read a book', 'I never put my faith', \"In a con man and his crooks I won't follow down that path\", 'And tempt the hands of fate', \"Aw, let's turn this thing around\", 'Before it gets too late [Chorus]', \"It's up to me and you\", 'Love can conquer hate', 'I know this to be true', \"That's what makes us great In the quiet of the night\", 'I lie here wide awake']\n"
     ]
    }
   ],
   "source": [
    "import re                  \n",
    "import numpy as np         \n",
    "import tensorflow as tf \n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/E06_LyricistAI/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 읽어오기\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1]\n",
      "They come from everywhere\n",
      "A longing to be free\n",
      "They come to join us here\n",
      "From sea to shining sea And they all have a dream\n",
      "As people always will\n",
      "To be safe and warm\n",
      "In that shining city on the hill Some wanna slam the door\n",
      "Instead of opening the gate\n",
      "Aw, let's turn this thing around\n",
      "Before it gets too late [Chorus]\n",
      "It's up to me and you\n",
      "Love can conquer hate\n",
      "I know this to be true\n",
      "That's what makes us great [Verse 2]\n",
      "Don't tell me a lie\n",
      "And sell it as a fact\n",
      "I've been down that road before\n",
      "And I ain't goin' back And don't you brag to me\n",
      "That you never read a book\n",
      "I never put my faith\n",
      "In a con man and his crooks I won't follow down that path\n",
      "And tempt the hands of fate\n",
      "Aw, let's turn this thing around\n",
      "Before it gets too late [Chorus]\n",
      "It's up to me and you\n",
      "Love can conquer hate\n",
      "I know this to be true\n",
      "That's what makes us great In the quiet of the night\n",
      "I lie here wide awake\n",
      "And I ask myself\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    #if sentence[0] == \"[\": continue   # 문장 처음이 '[' 인 문장은 건너뜁니다.\n",
    "    #if sentence[-1] == \"]\": continue  # 문장 끝이 ']' 인 문장은 건너뜁니다\n",
    "    if idx > 30: break   # 문장 30개만 확인\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화\n",
    "\n",
    "1. 문장 부호 양쪽에 공백을 추가\n",
    "2. 모든 문자들을 소문자로 변환\n",
    "3. 특수문자들은 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)       # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)              # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여줌\n",
    "    \n",
    "    if \"verse\" in sentence:\n",
    "        sentence = sentence.replace(\"verse\", \"\")\n",
    "    if \"chorus\" in sentence:\n",
    "        sentence = sentence.replace(\"chorus\", \"\")\n",
    "    \n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175950\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence) == 1: continue    \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "corpus.remove('<start>  <end>')         \n",
    "\n",
    "corpus[:30]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152493\n",
      "[[   2   45   66 ...    0    0    0]\n",
      " [   2    6 1690 ...    0    0    0]\n",
      " [   2 4692   16 ...    0    0    0]\n",
      " ...\n",
      " [   2    7 1019 ...    3    0    0]\n",
      " [   2   30   31 ...    0    0    0]\n",
      " [   5   32   15 ...   10   12    3]] <keras_preprocessing.text.Tokenizer object at 0x7f97eab76d50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있음. 이번에는 사용하지 안함\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 <unk> 처리\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 구축한 corpus로부터 Tokenizer가 사전을 자동구축\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환.\n",
    "   \n",
    "    for num in tensor:\n",
    "        if len(num) >= 29:\n",
    "            tensor = np.delete(tensor, num)\n",
    "            \n",
    "    print(len(tensor))\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "    \n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 어떻게 생겼는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  45  66  74 802   3   0   0   0   0   0   0   0   0]\n",
      "[ 45  66  74 802   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (121994, 14)\n",
      "Target Train: (121994, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습데이터 갯수가 124960개 이하이므로 이대로 학습을 시켜주겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)         \n",
    "BATCH_SIZE = 256                     \n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop  = tf.keras.layers.Dropout(0.5)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
       "array([[[-7.3142830e-05,  1.9110792e-04, -1.2877812e-04, ...,\n",
       "          1.4936674e-04, -6.2914289e-05,  7.7368190e-05],\n",
       "        [-3.0964537e-04,  1.0256601e-04, -1.0780464e-04, ...,\n",
       "          3.5763037e-04, -1.6552306e-04,  2.1371782e-04],\n",
       "        [-4.6166973e-04,  1.5614412e-04, -1.9737384e-04, ...,\n",
       "          5.2038813e-04, -3.7981090e-04,  4.0565775e-04],\n",
       "        ...,\n",
       "        [-4.8772686e-06, -2.6003379e-04,  6.4323787e-05, ...,\n",
       "         -4.7925822e-04,  1.5248383e-04,  2.4656547e-04],\n",
       "        [-1.8396150e-04, -4.3908483e-04,  4.0030832e-04, ...,\n",
       "         -4.3286735e-04,  6.2110316e-04,  5.9934106e-04],\n",
       "        [-3.7317694e-04, -6.4272358e-04,  7.5663044e-04, ...,\n",
       "         -3.9008658e-04,  1.0479168e-03,  9.1990089e-04]],\n",
       "\n",
       "       [[-7.3142830e-05,  1.9110792e-04, -1.2877812e-04, ...,\n",
       "          1.4936674e-04, -6.2914289e-05,  7.7368190e-05],\n",
       "        [ 3.5970126e-05,  2.6167880e-04, -6.3891319e-05, ...,\n",
       "          3.2870588e-04, -1.3344211e-04,  9.1092086e-05],\n",
       "        [ 2.1270302e-04,  2.7500471e-04,  4.9807357e-05, ...,\n",
       "          3.1651877e-04, -1.3419038e-04,  1.6435222e-06],\n",
       "        ...,\n",
       "        [-6.7245465e-04,  2.5979927e-04,  8.0734040e-05, ...,\n",
       "          4.4059558e-04,  7.8267651e-05,  1.1088101e-03],\n",
       "        [-8.4411638e-04, -4.6939218e-05,  3.4171136e-04, ...,\n",
       "          4.6056698e-04,  3.8405409e-04,  1.3596613e-03],\n",
       "        [-1.0159794e-03, -3.6230360e-04,  6.3927198e-04, ...,\n",
       "          4.5287205e-04,  6.9954857e-04,  1.5901107e-03]],\n",
       "\n",
       "       [[-7.3142830e-05,  1.9110792e-04, -1.2877812e-04, ...,\n",
       "          1.4936674e-04, -6.2914289e-05,  7.7368190e-05],\n",
       "        [-3.3605035e-04,  3.7382756e-04, -1.3111508e-04, ...,\n",
       "          3.1222127e-04,  4.7514852e-05,  8.9898655e-05],\n",
       "        [-4.7721947e-04,  2.9461581e-04, -2.7673121e-04, ...,\n",
       "          4.2084744e-04,  2.9122509e-04,  2.4702240e-04],\n",
       "        ...,\n",
       "        [-2.4922035e-04,  6.4934697e-04, -2.3050689e-04, ...,\n",
       "         -4.9437920e-04, -1.9985245e-04,  2.0445969e-04],\n",
       "        [-2.4934954e-04,  5.7009998e-04, -2.6534052e-04, ...,\n",
       "         -3.9215721e-04, -1.0884194e-05,  3.9278445e-04],\n",
       "        [-3.3617861e-04,  3.9843845e-04, -2.2481338e-04, ...,\n",
       "         -3.2863143e-04,  3.1568334e-04,  6.4482546e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-7.3142830e-05,  1.9110792e-04, -1.2877812e-04, ...,\n",
       "          1.4936674e-04, -6.2914289e-05,  7.7368190e-05],\n",
       "        [ 3.8011005e-04,  4.3664250e-04,  7.3261821e-05, ...,\n",
       "         -3.4867509e-05, -3.4201144e-05,  6.3760621e-05],\n",
       "        [ 6.4109982e-04,  2.1036812e-04,  4.9378921e-04, ...,\n",
       "         -3.8317314e-05,  2.8558152e-05, -7.0331887e-05],\n",
       "        ...,\n",
       "        [-1.4747726e-03, -1.0167327e-03,  2.2094816e-04, ...,\n",
       "          5.3990120e-04,  4.7677857e-04,  7.6342339e-04],\n",
       "        [-1.1095456e-03, -6.9845689e-04,  1.6984504e-04, ...,\n",
       "          7.7506376e-04,  2.8756270e-04,  5.9424242e-04],\n",
       "        [-9.5411832e-04, -4.8727516e-04,  1.5564285e-04, ...,\n",
       "          8.4108405e-04,  2.9466135e-04,  6.1011652e-04]],\n",
       "\n",
       "       [[-7.3142830e-05,  1.9110792e-04, -1.2877812e-04, ...,\n",
       "          1.4936674e-04, -6.2914289e-05,  7.7368190e-05],\n",
       "        [ 5.3114894e-05,  3.2366754e-04, -1.9866119e-04, ...,\n",
       "          2.8862094e-04, -6.2778650e-05,  3.9191375e-04],\n",
       "        [ 5.0243467e-05,  3.3972532e-04, -2.3007519e-04, ...,\n",
       "          2.9518953e-04, -4.7083490e-04,  6.0091075e-04],\n",
       "        ...,\n",
       "        [-3.4597731e-04, -2.0919419e-05, -2.4099551e-04, ...,\n",
       "          1.4492880e-03, -9.5265056e-04,  8.9902215e-04],\n",
       "        [-3.6781831e-04, -1.7427739e-04, -7.2249750e-06, ...,\n",
       "          1.3333097e-03, -5.2024360e-04,  1.1279603e-03],\n",
       "        [-4.4154373e-04, -3.7189704e-04,  2.8215145e-04, ...,\n",
       "          1.1931190e-03, -5.2282809e-05,  1.3600095e-03]],\n",
       "\n",
       "       [[-7.3142830e-05,  1.9110792e-04, -1.2877812e-04, ...,\n",
       "          1.4936674e-04, -6.2914289e-05,  7.7368190e-05],\n",
       "        [-3.7725427e-04,  3.0392944e-04, -2.3226351e-04, ...,\n",
       "          1.6107538e-04, -5.9878836e-05, -5.2149531e-05],\n",
       "        [-5.8334973e-04,  4.4220171e-04, -4.4093159e-04, ...,\n",
       "          1.9451596e-05, -3.3903369e-05, -5.1602797e-04],\n",
       "        ...,\n",
       "        [-6.4303720e-04, -2.9001219e-04, -1.2470470e-04, ...,\n",
       "          2.2745342e-04,  5.0777191e-04, -5.5785320e-04],\n",
       "        [-6.7244476e-04, -3.8774044e-04, -2.9890785e-05, ...,\n",
       "          3.1181064e-04,  7.1119727e-04, -2.4629658e-04],\n",
       "        [-7.3485932e-04, -5.4253655e-04,  1.1845402e-04, ...,\n",
       "          3.2531234e-04,  9.6725917e-04,  9.4767100e-05]]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 3.8058\n",
      "Epoch 2/30\n",
      "595/595 [==============================] - 87s 147ms/step - loss: 3.3388\n",
      "Epoch 3/30\n",
      "595/595 [==============================] - 88s 147ms/step - loss: 3.1604\n",
      "Epoch 4/30\n",
      "595/595 [==============================] - 88s 148ms/step - loss: 3.0328\n",
      "Epoch 5/30\n",
      "595/595 [==============================] - 88s 149ms/step - loss: 2.9238\n",
      "Epoch 6/30\n",
      "595/595 [==============================] - 89s 149ms/step - loss: 2.8262\n",
      "Epoch 7/30\n",
      "595/595 [==============================] - 89s 149ms/step - loss: 2.7360\n",
      "Epoch 8/30\n",
      "595/595 [==============================] - 89s 149ms/step - loss: 2.6511\n",
      "Epoch 9/30\n",
      "595/595 [==============================] - 89s 149ms/step - loss: 2.5726\n",
      "Epoch 10/30\n",
      "595/595 [==============================] - 90s 151ms/step - loss: 2.4973\n",
      "Epoch 11/30\n",
      "595/595 [==============================] - 89s 149ms/step - loss: 2.4267\n",
      "Epoch 12/30\n",
      "595/595 [==============================] - 87s 146ms/step - loss: 2.3600\n",
      "Epoch 13/30\n",
      "595/595 [==============================] - 85s 143ms/step - loss: 2.2945\n",
      "Epoch 14/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 2.2324\n",
      "Epoch 15/30\n",
      "595/595 [==============================] - 86s 144ms/step - loss: 2.1725\n",
      "Epoch 16/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 2.1140\n",
      "Epoch 17/30\n",
      "595/595 [==============================] - 86s 144ms/step - loss: 2.0581\n",
      "Epoch 18/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 2.0055\n",
      "Epoch 19/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 1.9538\n",
      "Epoch 20/30\n",
      "595/595 [==============================] - 87s 146ms/step - loss: 1.9051\n",
      "Epoch 21/30\n",
      "595/595 [==============================] - 85s 143ms/step - loss: 1.8580\n",
      "Epoch 22/30\n",
      "595/595 [==============================] - 85s 142ms/step - loss: 1.8136\n",
      "Epoch 23/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 1.7705\n",
      "Epoch 24/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 1.7311\n",
      "Epoch 25/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 1.6921\n",
      "Epoch 26/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 1.6559\n",
      "Epoch 27/30\n",
      "595/595 [==============================] - 86s 145ms/step - loss: 1.6219\n",
      "Epoch 28/30\n",
      "595/595 [==============================] - 87s 146ms/step - loss: 1.5888\n",
      "Epoch 29/30\n",
      "595/595 [==============================] - 86s 144ms/step - loss: 1.5578\n",
      "Epoch 30/30\n",
      "595/595 [==============================] - 86s 144ms/step - loss: 1.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f97ea783210>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954/954 - 9s - loss: 1.2473\n",
      "1.2472774982452393\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val,  dec_val, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so , <end> '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i look up at the beef , i m a posse <end> '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i look\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> that you re a liar , a liar <end> '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> that you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what makes the world go round ? <end> '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what makes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey bobby mcghee , yeah <end> '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hey \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey you know i got a <unk> like a <unk> <end> '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hey you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if you want it , come get it <end> '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> If you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [결과 및 회고]\n",
    "\n",
    "이번 프로젝트에서는 작사가 AI를 만들어 보았습니다.   \n",
    "결과적으로, __validation loss = 1.2472774982452393__ 까지 줄이는 모델을 만들 수 있었습니다.   \n",
    "또한, generate_text 함수를 사용해 실제로 문장이 문맥에 맞게 잘 형성되는지 확인해보았는데,   \n",
    "생각보다 작곡을 잘 해내서 놀랐습니다.   \n",
    "아래는 작곡의 예시입니다.    \n",
    "\n",
    "\"< start> what makes\"    -> '< start> what makes the world go round ? < end> '   \n",
    "\"< start> i love\"        -> '< start> i love you so , < end> '   \n",
    "\"< start> If you\"        -> '< start> if you want it , come get it < end> '   \n",
    "\n",
    "\n",
    "이번 프로젝트에서는 텍스트 데이터 전처리 과정의 중요성에 대해 배울 수 있었습니다.   \n",
    "전처리 과정에서 정규표현식을 사용하는 방법을 알게 되었고 (아직 익숙하지는 않지만...)\n",
    "노드에 나와있던 부분 이외에 처리해주어야 할 부분에 대해 더 생각해 볼 수 있었습니다. 저는 __[verse], [chorus]라고 쓰여있는 부분을 제거__ 하는 과정을 추가해주었습니다. \n",
    "프로젝트의 목표는 split 하기 전 전체 학습 데이터셋이 124960개 이하가 되도록 전처리하는 것이었는데,  \n",
    "처음엔 maxlen=15 값을 부여해 padding을 하면 맞출수 있다고 생각했으나 그래도 데이터는 14만개 이상이었습니다.\n",
    "그래서 padding하기 전, __너무 긴 문장은 제거해주는 과정을 추가__ 해주었고, 목표를 달성할 수 있었습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
